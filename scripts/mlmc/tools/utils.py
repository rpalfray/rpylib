import csv
import os
from pathlib import Path
from typing import Literal

import pandas
import pandas as pd
import pickle

from rpylib.model.utils import *
from rpylib.grid.spatial import CTMCGrid
from rpylib.model.levymodel.levymodel import LevyModel
from rpylib.model.levydrivensde.levydrivensde import LevyDrivenSDEModel
from rpylib.product.product import Product
from rpylib.tools.system import create_folder


cases_data = {"maturity": 3 / 12, "h0": 0.01, "mc_paths": 100_000, "max_level": 9}

clayton_copula = create_clayton_copula()

levy_models = {
    "hem": create_levy_model(ModelType.HEM)(),
    "vg": create_levy_model(ModelType.VG)(),
    "cgmy02": create_levy_model(ModelType.CGMY)(c=0.7, g=15, m=20, y=0.2),
    "cgmy04": create_levy_model(ModelType.CGMY)(c=0.7, g=15, m=20, y=0.4),
    "cgmy12": create_levy_model(ModelType.CGMY)(c=0.019, g=2, m=4, y=1.2),
    "cgmy15": create_levy_model(ModelType.CGMY)(c=0.007, g=2, m=4, y=1.5),
}

exp_of_levy_models = {
    "hem": create_exponential_of_levy_model(ModelType.HEM)(),
    "vg": create_exponential_of_levy_model(ModelType.VG)(),
    "cgmy02": create_exponential_of_levy_model(ModelType.CGMY)(
        c=1.230, g=15, m=20, y=0.2
    ),
    "cgmy04": create_exponential_of_levy_model(ModelType.CGMY)(
        c=0.700, g=15, m=20, y=0.4
    ),
    "cgmy11": create_exponential_of_levy_model(ModelType.CGMY)(
        c=0.025, g=2, m=4, y=1.1
    ),
    "cgmy12": create_exponential_of_levy_model(ModelType.CGMY)(
        c=0.019, g=2, m=4, y=1.2
    ),
    "cgmy15": create_exponential_of_levy_model(ModelType.CGMY)(
        c=0.007, g=2, m=4, y=1.5
    ),
}

calibrated_exp_of_levy_models = {
    key: run_default_calibration(model=val, maturity=cases_data["maturity"])
    for key, val in exp_of_levy_models.items()
}


def helper_create_results_folder(name: str, result_folder_name: str):
    # create the `results` folder
    create_folder(folder_name=result_folder_name)
    file_path = Path(result_folder_name, name + ".csv")
    data_path = Path(result_folder_name, name + "_data" + ".pkl")
    return file_path, data_path


def helper_data(
    name: str, model_case: Literal["1d", "copulas", "sde"], root_path: Path
) -> tuple[CTMCGrid, Union[LevyModel, LevyCopulaModel, LevyDrivenSDEModel], Product]:
    # retrieve the data generated by the `mlmc_convergence_***` script
    root_path_data = Path(root_path, "results/giles_convergence/" + model_case)
    file_name = name + "_data.pkl"

    with open(os.path.join(root_path_data, file_name), "rb") as f:
        data = pickle.load(f)

    grid, model, product = data["grid"], data["model"], data["product"]

    return grid, model, product


def save_mlmc_coupling_convergence_results(file_path, results, consistency_check, beta):
    with open(file_path, "w", newline="") as f:
        writer = csv.writer(f, delimiter=",", quoting=csv.QUOTE_NONE, escapechar=" ")
        writer.writerow(
            "level meanlevell varlevell consistencycheck kurtosis ml vl cl vl_slope ml_slope".split()
        )
        res_mlmc = results.mlmc_results
        levels = list(range(len(res_mlmc.mean_level_l)))
        rows = zip(
            levels,
            res_mlmc.mean_level_l,
            res_mlmc.var_level_l,
            consistency_check,
            res_mlmc.kurtosis,
            res_mlmc.ml,
            res_mlmc.vl,
            res_mlmc.cl,
        )

        # slope for vl and ml
        if len(res_mlmc.vl) > 4:
            myslice = slice(-4, None)  # last 4 elements
        else:
            myslice = slice(0, None)  # all elements
        mean_logvl = sum(np.log2(res_mlmc.vl[myslice])) / len(res_mlmc.vl[myslice])
        mean_logml = sum(np.log2(res_mlmc.ml[myslice])) / len(res_mlmc.ml[myslice])
        mean_levels = sum(levels[myslice]) / len(levels[myslice])

        for k, row in enumerate(rows):
            level = levels[k]
            slope_term_vl = 2 ** (
                -(2 - beta) * level + (mean_logvl + (2 - beta) * mean_levels)
            )
            slope_term_ml = 2 ** (
                -(1 - beta / 2) * level + (mean_logml + (1 - beta / 2) * mean_levels)
            )
            to_write = row + (slope_term_vl, slope_term_ml)
            writer.writerow(to_write)


def save_mlmc_coupling_applied_results(
    rmses, outputs, root_path_results, file_result_name, beta
):
    total_costs = []  # total cost for each model and rmse
    cost_cLs = []  # cost for the last level
    vl0s = []  # variance of the first level (level 0)
    all_Nls = []  # all number of Monte-Carlo paths Nl
    Nl0s = []
    for res in outputs:
        res_mlmc = res.mlmc_results
        total_costs.append(res_mlmc.cost)
        cost_cLs.append(res_mlmc.cl[-1])
        vl0s.append(res_mlmc.vl[0])
        all_Nls.append(res_mlmc.Nl)
        Nl0s.append(res_mlmc.Nl[0])

    os.makedirs(root_path_results, exist_ok=True)

    path_costs = os.path.join(root_path_results, file_result_name + "_costs.csv")
    normalised_costs = [rmse * rmse * total_costs[j] for j, rmse in enumerate(rmses)]
    normalised_costs_std_mc = [
        rmse * rmse * cost_cLs[j] * Nl0s[j] for j, rmse in enumerate(rmses)
    ]
    with open(path_costs, "a+", newline=""):
        try:
            df1 = pandas.read_csv(path_costs)
        except pd.errors.EmptyDataError:
            df1 = pd.DataFrame(
                columns=["rmse", "costxrmse2", "costxrmse2stdmc", "slope"], dtype=float
            )
        for rmse, cost, cost_std in zip(
            rmses, normalised_costs, normalised_costs_std_mc
        ):
            to_add = pd.DataFrame(
                {
                    "rmse": [rmse],
                    "costxrmse2": [cost],
                    "costxrmse2stdmc": [cost_std],
                    "slope": [0.0],
                }
            )
            if df1["rmse"].eq(rmse).any():
                df1.loc[df1["rmse"].eq(rmse)] = to_add.values
            else:
                df1 = pd.concat([df1, to_add], ignore_index=True)

        # recompute the slope curve
        slope = 0 if beta <= 1 else -4 * (beta - 1) / (2 - beta)
        all_rmses = df1["rmse"].values
        mean_logcost = sum(np.log(df1["costxrmse2"].values)) / len(all_rmses)
        mean_logrmse = sum(np.log(all_rmses)) / len(all_rmses)
        b_term = mean_logcost - slope * mean_logrmse
        df1["slope"] = np.exp(slope * np.log(all_rmses) + b_term)

        df1.sort_values(by="rmse", inplace=True)
        df1.to_csv(path_costs, index=False, sep=",", na_rep="nan")

    path_nls = os.path.join(root_path_results, file_result_name + "_Nls.csv")
    with open(path_nls, "a+", newline=""):
        maximum_level = max(len(elmt) for elmt in all_Nls)
        levels = list(range(maximum_level))
        try:
            df2 = pandas.read_csv(path_nls)
        except pd.errors.EmptyDataError:
            df2 = pd.DataFrame({"levels": levels}, dtype=int)
        for level in levels:
            if level not in df2["levels"].values.tolist():
                to_add_level = pd.DataFrame({"levels": [level]}, dtype=int)
                df2 = pd.concat([df2, to_add_level], ignore_index=True)
        maximum_level = max(maximum_level, df2.levels.size)
        for these_Nls, rmse in zip(all_Nls, rmses):
            rmse_str = str(rmse)
            df2[rmse_str] = list(these_Nls.astype(dtype=int)) + [
                np.nan for _ in range(len(list(these_Nls)), maximum_level)
            ]

        df2.sort_index(axis=1, inplace=True)
        # move last column 'levels' to first
        cols2 = list(df2.columns)
        cols2 = [cols2[-1]] + cols2[:-1]
        df2 = df2[cols2]
        df2.to_csv(path_nls, index=False, sep=",", na_rep="nan")
